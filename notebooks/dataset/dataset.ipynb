{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split = \"nuplan_private_test\"\n",
    "# dataset = ds.dataset(f\"/home/daniel/asim_workspace/data/{split}\", format=\"ipc\")\n",
    "\n",
    "# fragments = list(dataset.get_fragments())\n",
    "# print(f\"Number of files: {len(fragments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = \"asimtesting/nuplan_private_test/2021.07.25.16.16.23_veh-26_02446_02589.arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.fs as fs\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "s3_fs = fs.S3FileSystem(\n",
    "    access_key=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "    secret_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "    region=os.environ.get('AWS_DEFAULT_REGION')\n",
    ")\n",
    "# with s3_fs.open_input_file(s3_uri) as file:\n",
    "#     table = pa.ipc.open_file(file).read_all()\n",
    "#     print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset pointing to the specific file\n",
    "dataset = ds.dataset(f\"{s3_uri}\", format=\"ipc\", filesystem=s3_fs)\n",
    "\n",
    "# Get all column names and remove the ones you want to drop\n",
    "all_columns = dataset.schema.names\n",
    "columns_to_keep = [col for col in all_columns if col not in [\"front_cam_demo\", \"front_cam_transform\"]]\n",
    "\n",
    "# Load only the columns you want (more memory efficient)\n",
    "table = dataset.to_table(columns=columns_to_keep)\n",
    "\n",
    "# Save locally\n",
    "with pa.ipc.new_file(\"filtered_file.arrow\", table.schema) as writer:\n",
    "    writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the first 10 rows and only the columns you want\n",
    "  # Display first 10 rows\n",
    "# table = dataset.take([i for i in range(0, 100, 1)], columns=columns_to_keep)\n",
    "table = dataset.take([i for i in range(0, 100, 1)], columns=all_columns)\n",
    "\n",
    "# Save locally\n",
    "with pa.ipc.new_file(\"filtered_file_v2.arrow\", table.schema) as writer:\n",
    "    writer.write_table(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pa.memory_map(\"filtered_file_v2.arrow\", \"r\") as source:\n",
    "    mmap_table = pa.ipc.open_file(source).read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "from huggingface_hub import hf_hub_download\n",
    "from io import BytesIO\n",
    "\n",
    "# Download specific file from the repo\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"DanielDauner/delete_me\",\n",
    "    filename=\"2021.05.25.14.16.10_veh-35_01690_02183.arrow\",  # or whatever the file is named\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Read with PyArrow\n",
    "with pa.memory_map(file_path, \"r\") as source:\n",
    "    mmap_table = pa.ipc.open_file(source).read_all()\n",
    "\n",
    "# Create dataset and slice\n",
    "sliced_table = mmap_table.slice(0, 1000)  # First 1000 rows\n",
    "sliced_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
