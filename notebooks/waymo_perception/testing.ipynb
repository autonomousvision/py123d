{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from waymo_open_dataset import dataset_pb2\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from waymo_open_dataset import label_pb2\n",
    "from waymo_open_dataset.protos import camera_segmentation_pb2 as cs_pb2\n",
    "from waymo_open_dataset.utils import box_utils\n",
    "from waymo_open_dataset.utils.frame_utils import parse_range_image_and_camera_projection\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "WOPD_DATA_ROOT = Path(\"/media/nvme1/waymo_perception/training\")\n",
    "\n",
    "\n",
    "tfrecords_file_list = list(WOPD_DATA_ROOT.glob(\"*.tfrecord\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from d123.common.geometry.base import StateSE3\n",
    "from d123.common.geometry.bounding_box.bounding_box import BoundingBoxSE3\n",
    "\n",
    "\n",
    "# Frame attributes:\n",
    "#   context: <class 'waymo_open_dataset.dataset_pb2.Context'>\n",
    "#   timestamp_micros: <class 'int'>\n",
    "#   pose: <class 'waymo_open_dataset.dataset_pb2.Transform'>\n",
    "#   images: List with 5 images\n",
    "#   lasers: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 5\n",
    "#   laser_labels: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 0\n",
    "#   projected_lidar_labels: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 0\n",
    "#   camera_labels: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 0\n",
    "#   no_label_zones: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 0\n",
    "#   map_features: <class 'google._upb._message.RepeatedCompositeContainer'>\n",
    "#     Length: 0\n",
    "#   map_pose_offset: <class 'waymo_open_dataset.protos.vector_pb2.Vector3d'>\n",
    "\n",
    "file_idx = 0\n",
    "pathname = tfrecords_file_list[file_idx]\n",
    "dataset = tf.data.TFRecordDataset(pathname, compression_type=\"\")\n",
    "num_frames = sum(1 for _ in dataset)\n",
    "\n",
    "\n",
    "def read_jpg_image(data: bytes) -> np.ndarray:\n",
    "    \"\"\"Read a JPEG image from bytes and return it as a numpy array.\"\"\"\n",
    "    image = Image.open(io.BytesIO(data))\n",
    "    return np.array(image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ego_state_se3s = []\n",
    "front_images = []\n",
    "dataset = tf.data.TFRecordDataset(pathname, compression_type=\"\")\n",
    "\n",
    "boxes = []\n",
    "\n",
    "for frame_idx, data in enumerate(dataset):\n",
    "\n",
    "    frame = dataset_pb2.Frame()\n",
    "    frame.ParseFromString(data.numpy())\n",
    "    # print(frame.camera_labels)\n",
    "    for label in frame.laser_labels:\n",
    "        boxes.append(\n",
    "            BoundingBoxSE3(\n",
    "                center=StateSE3(\n",
    "                    x=label.box.center_x,\n",
    "                    y=label.box.center_y,\n",
    "                    z=label.box.center_z,\n",
    "                    pitch=0.0,\n",
    "                    roll=0.0,\n",
    "                    yaw=label.box.heading,\n",
    "                ),\n",
    "                length=label.box.length,\n",
    "                width=label.box.width,\n",
    "                height=label.box.height,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(frame.context)\n",
    "\n",
    "    # Print all attributes of the frame\n",
    "    # print(\"Frame attributes:\")\n",
    "    # for field in frame.DESCRIPTOR.fields:\n",
    "    #     field_name = field.name\n",
    "    #     if hasattr(frame, field_name):\n",
    "    #         value = getattr(frame, field_name)\n",
    "    #         if field_name != \"images\":  # Don't print the whole image data\n",
    "    #             print(f\"  {field_name}: {type(value)}\")\n",
    "    #             if hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)):\n",
    "    #                 print(f\"    Length: {len(value)}\")\n",
    "    #         else:\n",
    "    #             print(f\"  {field_name}: List with {len(value)} images\")\n",
    "\n",
    "    # Print information about the first image if available\n",
    "    # if frame.images:\n",
    "    #     print(\"\\nFirst image details:\")\n",
    "    #     first_image = frame.images[0]\n",
    "    #     for field in first_image.DESCRIPTOR.fields:\n",
    "    #         field_name = field.name\n",
    "    #         if hasattr(first_image, field_name):\n",
    "    #             value = getattr(first_image, field_name)\n",
    "    #             if field_name != \"image\":  # Don't print the binary data\n",
    "    #                 print(f\"  {field_name}: {value}\")\n",
    "    #             else:\n",
    "    #                 print(f\"  {field_name}: binary data of length {len(value)} bytes\")\n",
    "\n",
    "    # for image in frame.images:\n",
    "    # print(image.name)\n",
    "\n",
    "    # print([image.name for image in frame.images])\n",
    "    # print(frame.images[0])\n",
    "\n",
    "    # # 1. pose\n",
    "    pose = np.array(frame.pose.transform).reshape(4, 4)\n",
    "    yaw_pitch_roll = Quaternion(matrix=pose[:3, :3]).yaw_pitch_roll\n",
    "    ego_state_se3s.append(\n",
    "        np.array(\n",
    "            [\n",
    "                pose[0, 3],  # x\n",
    "                pose[1, 3],  # y\n",
    "                pose[2, 3],  # z\n",
    "                yaw_pitch_roll[2],  # yaw\n",
    "                yaw_pitch_roll[1],  # pitch\n",
    "                yaw_pitch_roll[0],  # roll\n",
    "            ],\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # # plt.show()\n",
    "    if frame_idx == 0:\n",
    "        break\n",
    "\n",
    "ego_state_se3s = np.array(ego_state_se3s, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx, data in enumerate(dataset):\n",
    "    frame = dataset_pb2.Frame()\n",
    "    frame.ParseFromString(data.numpy())\n",
    "    if frame_idx == 3:\n",
    "        break\n",
    "\n",
    "print(\"Ego\")\n",
    "ego_transform = np.array(frame.pose.transform).reshape(4, 4)\n",
    "print(ego_transform[:3, 3])\n",
    "\n",
    "print(\"Frame\")\n",
    "for image in frame.images:\n",
    "    image_transform = np.array(image.pose.transform).reshape(4, 4)\n",
    "    print(image.name, image_transform[:3, 3])\n",
    "\n",
    "print(\"Context\")\n",
    "for image in frame.context.camera_calibrations:\n",
    "    image_transform = np.array(image.extrinsic.transform).reshape(4, 4)\n",
    "    print(image.name, image_transform[:3, 3])\n",
    "\n",
    "# 1 [ 1.5441613  -0.02302364  2.11557864]\n",
    "# 2 [1.49672397 0.0954948  2.11616463]\n",
    "# 3 [ 1.49442485 -0.09637497  2.11519385]\n",
    "# 4 [1.43213651 0.11612398 2.11625087]\n",
    "# 5 [ 1.42936162 -0.11545043  2.1150792 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from d123.common.datatypes.time.time_point import TimePoint\n",
    "\n",
    "\n",
    "for frame_idx, data in enumerate(dataset):\n",
    "    frame = dataset_pb2.Frame()\n",
    "    frame.ParseFromString(data.numpy())\n",
    "    if frame_idx == 4:\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# for calibration in frame.context.camera_calibrations:\n",
    "\n",
    "frame.timestamp_micros, frame.images[0].pose_timestamp\n",
    "# frame.images[0]\n",
    "\n",
    "frame_timestamp = TimePoint.from_us(frame.timestamp_micros)\n",
    "image_timestamp = TimePoint.from_s(frame.images[0].pose_timestamp)\n",
    "frame_timestamp.time_s, frame_timestamp.time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = next((dataset_pb2.Frame().FromString(data.numpy()) or dataset_pb2.Frame() for data in dataset), None)\n",
    "# if frame is None:\n",
    "#     raise ValueError(f\"No frames found in TFRecord {tf_record_path}\")\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d123.common.datatypes.detection.detection_types import DetectionType\n",
    "from d123.common.visualization.color.default import BOX_DETECTION_CONFIG, EGO_VEHICLE_CONFIG\n",
    "from d123.common.visualization.matplotlib.observation import add_bounding_box_to_ax\n",
    "\n",
    "\n",
    "ego_rear_axle = StateSE3.from_array(ego_state_se3s[0])\n",
    "\n",
    "ego_rear_axle = StateSE3.from_array(np.zeros_like(ego_state_se3s[0]))\n",
    "\n",
    "ego_box = BoundingBoxSE3(center=ego_rear_axle, length=4.0, width=1.8, height=1.6)\n",
    "\n",
    "plot_config = BOX_DETECTION_CONFIG[DetectionType.VEHICLE]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "\n",
    "for box in boxes:\n",
    "    add_bounding_box_to_ax(ax, box, plot_config)\n",
    "\n",
    "\n",
    "add_bounding_box_to_ax(ax, ego_box, EGO_VEHICLE_CONFIG)\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(ego_state_se3s[:, 0], ego_state_se3s[:, 1])\n",
    "ax.set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Define the output video path\n",
    "output_video_path = str(f\"front_camera_video_{file_idx}.mp4\")\n",
    "\n",
    "# Get the dimensions of the first image\n",
    "height, width, channels = front_images[0].shape\n",
    "\n",
    "# Create the video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "fps = 10  # 10 frames per second as requested\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Write each frame to the video\n",
    "for img in tqdm(front_images, desc=\"Creating video\"):\n",
    "    # Convert from RGB to BGR (OpenCV uses BGR)\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    out.write(img_bgr)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()\n",
    "\n",
    "print(f\"Video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "import pandas as pd\n",
    "# parquet_file = \"/home/daniel/Downloads/testing_location_vehicle_pose_10084636266401282188_1120_000_1140_000.parquet\"\n",
    "\n",
    "parquet_file = \"/home/daniel/Downloads/validation_stats_10203656353524179475_7625_000_7645_000.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
