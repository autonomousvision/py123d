{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import LineString, Polygon, Point\n",
        "import numpy as np\n",
        "\n",
        "from typing import List\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from nuplan.database.nuplan_db_orm.nuplandb import LidarBox\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.ipc as ipc\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from asim.common.geometry.base import StateSE3\n",
        "from asim.common.geometry.bounding_box.bounding_box import BoundingBoxSE3\n",
        "from asim.common.geometry.constants import DEFAULT_ROLL, DEFAULT_PITCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from asim.dataset.maps.abstract_map import MapSurfaceType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nuplan.database.nuplan_db_orm.nuplandb import NuPlanDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 2021.08.17.16.57.11_veh-08_01200_01636 us-pa-pittsburgh-hazelwood\n"
          ]
        }
      ],
      "source": [
        "from asim.dataset.dataset_specific.nuplan.data_conversion import NuPlanDataset\n",
        "\n",
        "\n",
        "NUPLAN_DATA_ROOT = Path(os.environ[\"NUPLAN_DATA_ROOT\"])\n",
        "SPLIT_PATH = NUPLAN_DATA_ROOT / \"nuplan-v1.1\" / \"splits\" / \"mini\"\n",
        "\n",
        "\n",
        "db_files = list(SPLIT_PATH.iterdir())\n",
        "# idx = 0\n",
        "# for idx in range(len(db_files)):\n",
        "idx = 14\n",
        "\n",
        "log_db = NuPlanDB(NUPLAN_DATA_ROOT, str(db_files[idx]), None)\n",
        "print(idx, log_db.log_name, log_db.log.map_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import asim\n",
        "\n",
        "\n",
        "print(asim.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20166 green\n",
            "18404 red\n",
            "18405 red\n",
            "20169 red\n",
            "20170 red\n",
            "20168 red\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "lidar_pc_token = \"b1459a3e87ec58fd\"\n",
        "\n",
        "all_lidar_pc_tokens = [lidar_pc.token for lidar_pc in log_db.lidar_pc]\n",
        "# print(all_lidar_pc_tokens)\n",
        "\n",
        "# 1. Scenario Tag\n",
        "# log_db.scenario_tag.select_one(lidar_pc_token=lidar_pc_token).type\n",
        "\n",
        "\n",
        "# 2. traffic lights\n",
        "traffic_lights = log_db.traffic_light_status.select_many(lidar_pc_token=all_lidar_pc_tokens[1])\n",
        "for traffic_light in traffic_lights:\n",
        "    print(int(traffic_light.lane_connector_id), traffic_light.status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from asim.dataset.observation.agent_datatypes import BoundingBoxType\n",
        "\n",
        "\n",
        "int(BoundingBoxType.CZONE_SIGN.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from asim.dataset.observation.agent_datatypes import BoundingBoxType\n",
        "\n",
        "\n",
        "name_mapping = {\n",
        "    \"vehicle\": BoundingBoxType.VEHICLE,\n",
        "    \"bicycle\": BoundingBoxType.BICYCLE,\n",
        "    \"pedestrian\": BoundingBoxType.PEDESTRIAN,\n",
        "    \"traffic_cone\": BoundingBoxType.TRAFFIC_CONE,\n",
        "    \"barrier\": BoundingBoxType.BARRIER,\n",
        "    \"czone_sign\": BoundingBoxType.CZONE_SIGN,\n",
        "    \"generic_object\": BoundingBoxType.GENERIC_OBJECT,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 9/8720 [00:00<00:39, 218.56it/s]\n"
          ]
        }
      ],
      "source": [
        "from nuplan.common.geometry.compute import get_pacifica_parameters\n",
        "\n",
        "\n",
        "log_name = log_db.log_name\n",
        "log_token = log_db.log.token\n",
        "map_location = log_db.log.map_version\n",
        "vehicle_name = log_db.log.vehicle_name\n",
        "\n",
        "\n",
        "time_us_log: List[int] = []\n",
        "\n",
        "bb_ego_log: List[List[float]] = []\n",
        "bb_frame_log: List[List[List[float]]] = []\n",
        "bb_track_log: List[List[str]] = []\n",
        "bb_types_log: List[List[int]] = []\n",
        "\n",
        "ego_states_log: List[List[float]] = []\n",
        "\n",
        "\n",
        "for lidar_pc in tqdm(log_db.lidar_pc, dynamic_ncols=True):\n",
        "    # 1. time_us\n",
        "    time_us_log.append(lidar_pc.timestamp)\n",
        "\n",
        "    bb_frame: List[List[float]] = []\n",
        "    bb_track: List[str] = []\n",
        "    bb_types: List[int] = []\n",
        "\n",
        "    for lidar_box in lidar_pc.lidar_boxes:\n",
        "        lidar_box: LidarBox\n",
        "        center = StateSE3(\n",
        "            x=lidar_box.x,\n",
        "            y=lidar_box.y,\n",
        "            z=lidar_box.z,\n",
        "            roll=DEFAULT_ROLL,\n",
        "            pitch=DEFAULT_PITCH,\n",
        "            yaw=lidar_box.yaw,\n",
        "        )\n",
        "        bounding_box_se3 = BoundingBoxSE3(center, lidar_box.length, lidar_box.width, lidar_box.height)\n",
        "\n",
        "        bb_frame.append(pa.array(bounding_box_se3.array))\n",
        "        bb_track.append(lidar_box.track_token)\n",
        "        bb_types.append(int(name_mapping[lidar_box.category.name]))\n",
        "\n",
        "    bb_frame_log.append(bb_frame)\n",
        "    bb_track_log.append(bb_track)\n",
        "    bb_types_log.append(bb_types)\n",
        "\n",
        "    # 2. ego_states\n",
        "    yaw, pitch, roll = yaw_pitch_roll = lidar_pc.ego_pose.quaternion.yaw_pitch_roll\n",
        "    vehicle_parameters = get_pacifica_parameters()\n",
        "    ego_bounding_box_se3 = BoundingBoxSE3(\n",
        "        center=StateSE3(\n",
        "            x=lidar_pc.ego_pose.x,\n",
        "            y=lidar_pc.ego_pose.y,\n",
        "            z=lidar_pc.ego_pose.z,\n",
        "            roll=roll,\n",
        "            pitch=pitch,\n",
        "            yaw=yaw,\n",
        "        ),\n",
        "        length=vehicle_parameters.length,\n",
        "        width=vehicle_parameters.width,\n",
        "        height=vehicle_parameters.height,\n",
        "    )\n",
        "\n",
        "    bb_ego_log.append(pa.array(ego_bounding_box_se3.array))\n",
        "\n",
        "    if len(bb_ego_log) > 9:\n",
        "        break\n",
        "    # break\n",
        "\n",
        "\n",
        "# Option 1: List Column Approach\n",
        "list_data = {\"time_us\": time_us_log, \"bb_frame\": bb_frame_log, \"bb_track\": bb_track_log, \"bb_types\": bb_types_log, \"bb_ego\": bb_ego_log}\n",
        "\n",
        "# Create a PyArrow Table\n",
        "list_schema = pa.schema(\n",
        "    [\n",
        "        (\"time_us\", pa.int64()),\n",
        "        (\"bb_frame\", pa.list_(pa.list_(pa.float64(), 9))),\n",
        "        (\"bb_track\", pa.list_(pa.string())),\n",
        "        (\"bb_types\", pa.list_(pa.int32())),\n",
        "        (\"bb_ego\", pa.list_(pa.float64(), 9)),\n",
        "    ]\n",
        ")\n",
        "list_table = pa.Table.from_pydict(list_data, schema=list_schema)\n",
        "\n",
        "\n",
        "metadata = {\n",
        "    \"recording_id\": \"drive_20250515_001\",\n",
        "    \"location\": \"Mountain View, CA\",\n",
        "    \"weather\": \"sunny\",\n",
        "    \"sensor_config\": \"standard_suite_v3\"\n",
        "}\n",
        "metadata_fields = []\n",
        "metadata_values = []\n",
        "for key, value in metadata.items():\n",
        "    metadata_fields.append(key)\n",
        "    metadata_values.append(pa.scalar(value))\n",
        "\n",
        "metadata_table = pa.Table.from_arrays(\n",
        "    [pa.array([value]) for value in metadata_values],\n",
        "    metadata_fields\n",
        ")\n",
        "\n",
        "# schema = {\n",
        "#     \"timeseries\": list_table.schema,\n",
        "#     \"metadata\": metadata_table.schema\n",
        "# }\n",
        "# schema_batch = pa.record_batch([pa.array([str(schema)])], [\"schema\"])\n",
        "\n",
        "# # Write to Arrow file\n",
        "# # with pa.OSFile(f\"{log_name}.arrow\", \"wb\") as sink:\n",
        "# #     writer = pa.RecordBatchFileWriter(sink, list_table.schema)\n",
        "# #     writer.write_table(list_table)\n",
        "# #     writer.close()\n",
        "# schema_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.ipc as ipc\n",
        "import mmap\n",
        "import struct\n",
        "\n",
        "def save_arrow_tables(tables_dict, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        # Write header: number of tables\n",
        "        f.write(struct.pack('<I', len(tables_dict)))\n",
        "        \n",
        "        # Write table of contents (TOC)\n",
        "        toc_start = f.tell()\n",
        "        toc_size = len(tables_dict) * (4 + 8 + 8)  # name_len + offset + size\n",
        "        f.seek(toc_start + toc_size)  # Skip TOC space for now\n",
        "        \n",
        "        toc_entries = []\n",
        "        for name, table in tables_dict.items():\n",
        "            offset = f.tell()\n",
        "            \n",
        "            # Write table using Arrow IPC\n",
        "            sink = pa.BufferOutputStream()\n",
        "            with ipc.new_file(sink, table.schema) as writer:\n",
        "                writer.write_table(table)\n",
        "            \n",
        "            buffer = sink.getvalue()\n",
        "            f.write(buffer.to_pybytes())\n",
        "            \n",
        "            toc_entries.append((name, offset, len(buffer)))\n",
        "        \n",
        "        # Write TOC at the beginning\n",
        "        current_pos = f.tell()\n",
        "        f.seek(toc_start)\n",
        "        for name, offset, size in toc_entries:\n",
        "            name_bytes = name.encode('utf-8')\n",
        "            f.write(struct.pack('<I', len(name_bytes)))\n",
        "            f.write(name_bytes)\n",
        "            f.write(struct.pack('<Q', offset))\n",
        "            f.write(struct.pack('<Q', size))\n",
        "        \n",
        "        f.seek(current_pos)\n",
        "\n",
        "def load_arrow_table(filename, table_name):\n",
        "    with open(filename, 'rb') as f:\n",
        "        # Read number of tables\n",
        "        num_tables = struct.unpack('<I', f.read(4))[0]\n",
        "        \n",
        "        # Read TOC to find our table\n",
        "        for _ in range(num_tables):\n",
        "            name_len = struct.unpack('<I', f.read(4))[0]\n",
        "            name = f.read(name_len).decode('utf-8')\n",
        "            offset = struct.unpack('<Q', f.read(8))[0]\n",
        "            size = struct.unpack('<Q', f.read(8))[0]\n",
        "            \n",
        "            if name == table_name:\n",
        "                # Memory map the specific table section\n",
        "                with open(filename, 'rb') as mf:\n",
        "                    with mmap.mmap(mf.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "                        table_data = mm[offset:offset + size]\n",
        "                        reader = ipc.open_file(pa.py_buffer(table_data))\n",
        "                        return reader.read_all()\n",
        "    \n",
        "    raise KeyError(f\"Table '{table_name}' not found\")\n",
        "\n",
        "def list_tables(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        num_tables = struct.unpack('<I', f.read(4))[0]\n",
        "        tables = []\n",
        "        \n",
        "        for _ in range(num_tables):\n",
        "            name_len = struct.unpack('<I', f.read(4))[0]\n",
        "            name = f.read(name_len).decode('utf-8')\n",
        "            f.read(16)  # Skip offset and size\n",
        "            tables.append(name)\n",
        "        \n",
        "        return tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usage\n",
        "tables = {'table1': list_table, 'table2': metadata_table}\n",
        "save_arrow_tables(tables, 'multi_table.arrow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyarrow.Table\n",
              "recording_id: string\n",
              "location: string\n",
              "weather: string\n",
              "sensor_config: string\n",
              "----\n",
              "recording_id: [[\"drive_20250515_001\"]]\n",
              "location: [[\"Mountain View, CA\"]]\n",
              "weather: [[\"sunny\"]]\n",
              "sensor_config: [[\"standard_suite_v3\"]]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_arrow_table(\"multi_table.arrow\", \"table2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['table1', 'table2', 'table3']\n"
          ]
        }
      ],
      "source": [
        "class ArrowMultiTableFile:\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        self._file = open(filename, 'rb')\n",
        "        self._mmap = mmap.mmap(self._file.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "        self._parse_toc()\n",
        "    \n",
        "    def _parse_toc(self):\n",
        "        self._tables = {}\n",
        "        self._mmap.seek(0)\n",
        "        num_tables = struct.unpack('<I', self._mmap.read(4))[0]\n",
        "        \n",
        "        for _ in range(num_tables):\n",
        "            name_len = struct.unpack('<I', self._mmap.read(4))[0]\n",
        "            name = self._mmap.read(name_len).decode('utf-8')\n",
        "            offset = struct.unpack('<Q', self._mmap.read(8))[0]\n",
        "            size = struct.unpack('<Q', self._mmap.read(8))[0]\n",
        "            self._tables[name] = (offset, size)\n",
        "    \n",
        "    def get_table(self, name):\n",
        "        if name not in self._tables:\n",
        "            raise KeyError(f\"Table '{name}' not found\")\n",
        "        \n",
        "        offset, size = self._tables[name]\n",
        "        table_data = self._mmap[offset:offset + size]\n",
        "        reader = ipc.open_file(pa.py_buffer(table_data))\n",
        "        return reader.read_all()\n",
        "    \n",
        "    def list_tables(self):\n",
        "        return list(self._tables.keys())\n",
        "    \n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.close()\n",
        "    \n",
        "    def close(self):\n",
        "        if hasattr(self, '_mmap'):\n",
        "            self._mmap.close()\n",
        "        if hasattr(self, '_file'):\n",
        "            self._file.close()\n",
        "\n",
        "# Usage\n",
        "tables = {'table1': list_table, 'table2': metadata_table, 'table3': metadata_table}\n",
        "save_arrow_tables(tables, 'multi_table.arrow')\n",
        "\n",
        "# Memory-mapped reading\n",
        "with ArrowMultiTableFile('multi_table.arrow') as reader:\n",
        "    table1 = reader.get_table('table1')  # Memory mapped!\n",
        "    table2 = reader.get_table('table2')  # Memory mapped!\n",
        "    table3 = reader.get_table('table2')  # Memory mapped!\n",
        "    print(reader.list_tables())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyarrow.Table\n",
              "recording_id: string\n",
              "location: string\n",
              "weather: string\n",
              "sensor_config: string\n",
              "----\n",
              "recording_id: [[\"drive_20250515_001\"]]\n",
              "location: [[\"Mountain View, CA\"]]\n",
              "weather: [[\"sunny\"]]\n",
              "sensor_config: [[\"standard_suite_v3\"]]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "asim",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
